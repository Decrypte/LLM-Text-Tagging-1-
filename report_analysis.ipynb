{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (100, 73)\n",
      "\n",
      "Data types:\n",
      "VIN                                          object\n",
      "TRANSACTION_ID                                int64\n",
      "CORRECTION_VERBATIM                          object\n",
      "CUSTOMER_VERBATIM                            object\n",
      "REPAIR_DATE                          datetime64[ns]\n",
      "CAUSAL_PART_NM                               object\n",
      "GLOBAL_LABOR_CODE_DESCRIPTION                object\n",
      "PLATFORM                                     object\n",
      "BODY_STYLE                                   object\n",
      "VPPC                                         object\n",
      "PLANT                                        object\n",
      "BUILD_COUNTRY                                object\n",
      "LAST_KNOWN_DLR_NAME                          object\n",
      "LAST_KNOWN_DLR_CITY                          object\n",
      "REPAIRING_DEALER_CODE                        object\n",
      "DEALER_NAME                                  object\n",
      "REPAIR_DLR_CITY                              object\n",
      "STATE                                        object\n",
      "DEALER_REGION                                 int64\n",
      "REPAIR_DLR_POSTAL_CD                         object\n",
      "REPAIR_AGE                                    int64\n",
      "KM                                            int64\n",
      "COMPLAINT_CD_CSI                              int64\n",
      "COMPLAINT_CD                                 object\n",
      "VEH_TEST_GRP                                 object\n",
      "COUNTRY_SALE_ISO                             object\n",
      "ORD_SELLING_SRC_CD                            int64\n",
      "OPTN_FAMLY_CERTIFICATION                     object\n",
      "OPTF_FAMLY_EMISSIOF_SYSTEM                   object\n",
      "GLOBAL_LABOR_CODE                             int64\n",
      "TRANSACTION_CATEGORY                         object\n",
      "CAMPAIGN_NBR                                float64\n",
      "REPORTING_COST                              float64\n",
      "TOTALCOST                                   float64\n",
      "LBRCOST                                     float64\n",
      "ENGINE                                       object\n",
      "ENGINE_DESC                                  object\n",
      "TRANSMISSION                                 object\n",
      "TRANSMISSION_DESC                            object\n",
      "ENGINE_SOURCE_PLANT                          object\n",
      "ENGINE_TRACE_NBR                             object\n",
      "TRANSMISSION_SOURCE_PLANT                     int64\n",
      "TRANSMISSION_TRACE_NBR                       object\n",
      "SRC_TXN_ID                                    int64\n",
      "SRC_VER_NBR                                   int64\n",
      "TRANSACTION_CNTR                              int64\n",
      "MEDIA_FLAG                                   object\n",
      "VIN_MODL_DESGTR                              object\n",
      "LINE_SERIES                                  object\n",
      "LAST_KNOWN_DELVRY_TYPE_CD                     int64\n",
      "NON_CAUSAL_PART_QTY                           int64\n",
      "SALES_REGION_CODE                             int64\n",
      "TRANSACTION_ID_OUTLIER                         bool\n",
      "DEALER_REGION_OUTLIER                          bool\n",
      "REPAIR_AGE_OUTLIER                             bool\n",
      "KM_OUTLIER                                     bool\n",
      "GLOBAL_LABOR_CODE_OUTLIER                      bool\n",
      "REPORTING_COST_OUTLIER                         bool\n",
      "TOTALCOST_OUTLIER                              bool\n",
      "LBRCOST_OUTLIER                                bool\n",
      "TRANSMISSION_SOURCE_PLANT_OUTLIER              bool\n",
      "SRC_VER_NBR_OUTLIER                            bool\n",
      "LAST_KNOWN_DELVRY_TYPE_CD_OUTLIER              bool\n",
      "NON_CAUSAL_PART_QTY_OUTLIER                    bool\n",
      "SALES_REGION_CODE_OUTLIER                      bool\n",
      "ISSUES                                       object\n",
      "COMPONENTS                                   object\n",
      "ACTIONS                                      object\n",
      "REPAIR_COMPLEXITY                            object\n",
      "VEHICLE_SYSTEM                               object\n",
      "FAILURE_MODE                                 object\n",
      "REPAIR_URGENCY                               object\n",
      "DIAGNOSTIC_METHOD                            object\n",
      "dtype: object\n",
      "\n",
      "Missing values summary:\n",
      "                            Missing Values  Percentage\n",
      "CAMPAIGN_NBR                           100       100.0\n",
      "ENGINE_SOURCE_PLANT                     12        12.0\n",
      "TRANSMISSION_TRACE_NBR                  12        12.0\n",
      "ENGINE_TRACE_NBR                        12        12.0\n",
      "OPTN_FAMLY_CERTIFICATION                10        10.0\n",
      "OPTF_FAMLY_EMISSIOF_SYSTEM               5         5.0\n",
      "CAUSAL_PART_NM                           5         5.0\n",
      "STATE                                    2         2.0\n",
      "VEH_TEST_GRP                             2         2.0\n",
      "REPAIR_DLR_POSTAL_CD                     2         2.0\n",
      "PLANT                                    1         1.0\n",
      "LINE_SERIES                              1         1.0\n",
      "\n",
      "Numerical columns summary:\n",
      "       TRANSACTION_ID          REPAIR_DATE  DEALER_REGION  REPAIR_AGE             KM  COMPLAINT_CD_CSI  ORD_SELLING_SRC_CD  GLOBAL_LABOR_CODE  CAMPAIGN_NBR  REPORTING_COST    TOTALCOST      LBRCOST  TRANSMISSION_SOURCE_PLANT    SRC_TXN_ID  SRC_VER_NBR  TRANSACTION_CNTR  LAST_KNOWN_DELVRY_TYPE_CD  NON_CAUSAL_PART_QTY  SALES_REGION_CODE\n",
      "count      100.000000                  100      100.00000  100.000000     100.000000             100.0          100.000000         100.000000           0.0      100.000000   100.000000   100.000000               1.000000e+02  1.000000e+02   100.000000             100.0                 100.000000           100.000000          100.00000\n",
      "mean     13036.900000  2024-01-22 07:26:24        1.09000   14.940000   24914.230000               0.0           24.590000         251.900000           NaN      531.193200   554.925900   106.344900               1.484867e+08  2.815767e+09     5.720000               1.0                  14.050000             0.070000            1.09000\n",
      "min      13021.000000  2024-01-02 00:00:00        1.00000    0.000000       3.000000               0.0           11.000000          20.000000           NaN       27.690000    27.690000    20.000000               2.878270e+05  2.808842e+09     2.000000               1.0                  10.000000             0.000000            1.00000\n",
      "25%      13027.750000  2024-01-12 00:00:00        1.00000    5.000000    8883.250000               0.0           13.000000         130.000000           NaN      305.432500   349.940000    61.855000               2.878270e+05  2.809436e+09     4.000000               1.0                  10.000000             0.000000            1.00000\n",
      "50%      13036.000000  2024-01-24 12:00:00        1.00000   12.000000   21962.000000               0.0           13.000000         130.000000           NaN      433.970000   457.225000    78.560000               8.042172e+06  2.820097e+09     4.000000               1.0                  10.000000             0.000000            1.00000\n",
      "75%      13041.250000  2024-02-01 00:00:00        1.00000   21.000000   35493.250000               0.0           48.000000         130.000000           NaN      554.062500   572.772500   108.055000               1.774929e+07  2.820880e+09     6.000000               1.0                  16.000000             0.000000            1.00000\n",
      "max      13081.000000  2024-02-07 00:00:00        4.00000   50.000000  107905.000000               0.0           72.000000        2400.000000           NaN     2457.450000  3205.450000  1012.670000               8.282984e+08  2.823000e+09    26.000000               1.0                  37.000000             1.000000            4.00000\n",
      "std         12.028166                  NaN        0.51434   12.367945   20747.078206               0.0           17.822976         546.451722           NaN      411.161608   439.561893   113.223074               3.079421e+08  5.790727e+06     4.040402               0.0                   6.652067             0.256432            0.51434\n",
      "\n",
      "Unique values in VIN: 98\n",
      "VIN\n",
      "3HRS9EED0LH255650    2\n",
      "1HRFFHEL8RZ133325    2\n",
      "1HRFFEE8XSZ230636    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Unique values in CORRECTION_VERBATIM: 93\n",
      "CORRECTION_VERBATIM\n",
      "REPLACED STEERING WHEEL       3\n",
      "Steering Wheel Replacement    3\n",
      "Replaced steering wheel       3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Unique values in CUSTOMER_VERBATIM: 100\n",
      "CUSTOMER_VERBATIM\n",
      "STEERING WHEEL COMING APART                                                                                              1\n",
      "CUSTOMER STATES HEATED STEERING WHEEL INOP                                                                               1\n",
      "OWNER REPORTS: THE SUPER CRUISE BAR ON THE STEERING WHEEL IS COMING OF F. CHECK AND ADVISE. ADVISOR RUNNING PRA TOOL.    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Unique values in CAUSAL_PART_NM: 18\n",
      "CAUSAL_PART_NM\n",
      "WHEEL ASM-STRG *JET BLACK       45\n",
      "WHEEL ASM-STRG *BLACK           12\n",
      "WHEEL ASM-STRG *VERY DARK AT     7\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Unique values in GLOBAL_LABOR_CODE_DESCRIPTION: 4\n",
      "GLOBAL_LABOR_CODE_DESCRIPTION\n",
      "Steering Wheel Replacement                  78\n",
      "Steering Wheel Spoke Cover Replacement      11\n",
      "Heated Steering Wheel Module Replacement     6\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy import stats\n",
    "import re\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "\n",
    "# Set display options\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "pd.set_option(\"display.width\", 1000)\n",
    "\n",
    "# Load the data\n",
    "filepath = r\"D:\\#Data\\Assessments\\Axion Ray\\Task 2.xlsx\"\n",
    "df = pd.read_excel(filepath)\n",
    "\n",
    "# Check basic info\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(\"\\nData types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percent = (missing_values / len(df)) * 100\n",
    "missing_df = pd.DataFrame(\n",
    "    {\"Missing Values\": missing_values, \"Percentage\": missing_percent}\n",
    ")\n",
    "print(\"\\nMissing values summary:\")\n",
    "print(\n",
    "    missing_df[missing_df[\"Missing Values\"] > 0].sort_values(\n",
    "        \"Percentage\", ascending=False\n",
    "    )\n",
    ")\n",
    "\n",
    "# Basic statistics for numerical columns\n",
    "print(\"\\nNumerical columns summary:\")\n",
    "print(df.describe())\n",
    "\n",
    "# Check unique values for categorical columns\n",
    "categorical_cols = df.select_dtypes(include=[\"object\"]).columns\n",
    "for col in categorical_cols[:5]:  # Showing first 5 to keep output manageable\n",
    "    print(f\"\\nUnique values in {col}: {df[col].nunique()}\")\n",
    "    print(df[col].value_counts().head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 columns by importance score:\n",
      "CORRECTION_VERBATIM      0.733256\n",
      "CUSTOMER_VERBATIM        0.668245\n",
      "VIN                      0.544583\n",
      "LAST_KNOWN_DLR_CITY      0.540320\n",
      "REPAIRING_DEALER_CODE    0.538206\n",
      "REPAIR_DLR_CITY          0.537325\n",
      "REPAIR_DLR_POSTAL_CD     0.528843\n",
      "LAST_KNOWN_DLR_NAME      0.518129\n",
      "DEALER_NAME              0.516282\n",
      "ENGINE_TRACE_NBR         0.507318\n",
      "Name: composite_score, dtype: float64\n",
      "\n",
      "Identified 5 most critical columns:\n",
      "['CORRECTION_VERBATIM', 'CUSTOMER_VERBATIM', 'VIN', 'LAST_KNOWN_DLR_CITY', 'REPAIRING_DEALER_CODE']\n",
      "\n",
      "Justification for critical column selection:\n",
      "\n",
      "CORRECTION_VERBATIM:\n",
      "  - Missing values: 0.00%\n",
      "  - Average word count: 18.32\n",
      "  - Unique values: 93\n",
      "  - Correlation/MI with REPAIR_AGE: 0.0121\n",
      "  - Correlation/MI with TOTALCOST: 0.0043\n",
      "  - Correlation/MI with LBRCOST: 0.0017\n",
      "  - Composite importance score: 0.7333\n",
      "\n",
      "CUSTOMER_VERBATIM:\n",
      "  - Missing values: 0.00%\n",
      "  - Average word count: 15.57\n",
      "  - Unique values: 100\n",
      "  - Composite importance score: 0.6682\n",
      "\n",
      "VIN:\n",
      "  - Missing values: 0.00%\n",
      "  - Average word count: 1.00\n",
      "  - Unique values: 98\n",
      "  - Correlation/MI with REPAIR_AGE: 0.0094\n",
      "  - Correlation/MI with TOTALCOST: 0.0047\n",
      "  - Correlation/MI with LBRCOST: 0.0016\n",
      "  - Composite importance score: 0.5446\n",
      "\n",
      "LAST_KNOWN_DLR_CITY:\n",
      "  - Missing values: 0.00%\n",
      "  - Average word count: 1.34\n",
      "  - Unique values: 94\n",
      "  - Correlation/MI with REPAIR_AGE: 0.0154\n",
      "  - Correlation/MI with TOTALCOST: 0.0049\n",
      "  - Correlation/MI with LBRCOST: 0.0017\n",
      "  - Composite importance score: 0.5403\n",
      "\n",
      "REPAIRING_DEALER_CODE:\n",
      "  - Missing values: 0.00%\n",
      "  - Average word count: 1.00\n",
      "  - Unique values: 95\n",
      "  - Correlation/MI with REPAIR_AGE: 0.0133\n",
      "  - Correlation/MI with TOTALCOST: 0.0058\n",
      "  - Correlation/MI with LBRCOST: 0.0016\n",
      "  - Composite importance score: 0.5382\n"
     ]
    }
   ],
   "source": [
    "def analyze_column_importance(df):\n",
    "    \"\"\"\n",
    "    Analyze the importance of columns based on multiple metrics\n",
    "    \"\"\"\n",
    "    # Initialize a DataFrame to store importance metrics\n",
    "    importance_df = pd.DataFrame(index=df.columns)\n",
    "\n",
    "    # Calculate missing value percentages\n",
    "    importance_df[\"missing_percentage\"] = df.isnull().mean() * 100\n",
    "\n",
    "    # Calculate cardinality (number of unique values)\n",
    "    importance_df[\"cardinality\"] = df.apply(lambda x: x.nunique())\n",
    "    importance_df[\"cardinality_ratio\"] = importance_df[\"cardinality\"] / len(df)\n",
    "\n",
    "    # Calculate entropy for categorical columns\n",
    "    def calculate_entropy(column):\n",
    "        if column.dtype == \"object\":\n",
    "            vc = column.value_counts(normalize=True, dropna=True)\n",
    "            return stats.entropy(vc)\n",
    "        return np.nan\n",
    "\n",
    "    importance_df[\"entropy\"] = df.apply(calculate_entropy)\n",
    "\n",
    "    # Analyze text richness for text columns\n",
    "    def text_richness(column):\n",
    "        if column.dtype == \"object\":\n",
    "            # Check if column contains text data\n",
    "            sample = column.dropna().sample(min(100, len(column.dropna()))).astype(str)\n",
    "            avg_words = sample.str.split().str.len().mean()\n",
    "            return avg_words\n",
    "        return np.nan\n",
    "\n",
    "    importance_df[\"avg_word_count\"] = df.apply(text_richness)\n",
    "\n",
    "    # Calculate correlation with numerical target columns like REPAIR_AGE, TOTALCOST\n",
    "    target_cols = [\"REPAIR_AGE\", \"TOTALCOST\", \"LBRCOST\"]\n",
    "    for target in target_cols:\n",
    "        if target in df.columns:\n",
    "            # For numerical columns\n",
    "            numerical_cols = df.select_dtypes(include=np.number).columns\n",
    "            corr_dict = {}\n",
    "            for col in numerical_cols:\n",
    "                if col != target:\n",
    "                    corr_dict[col] = df[[col, target]].corr().iloc[0, 1]\n",
    "\n",
    "            # For categorical columns, use mutual information\n",
    "            categorical_cols = df.select_dtypes(include=[\"object\"]).columns\n",
    "            for col in categorical_cols:\n",
    "                if df[col].nunique() < 100:  # Skip high cardinality categorical\n",
    "                    try:\n",
    "                        # Convert to dummies for MI calculation\n",
    "                        dummies = pd.get_dummies(df[col], drop_first=True)\n",
    "                        X = (\n",
    "                            dummies.values\n",
    "                            if dummies.shape[1] > 0\n",
    "                            else np.zeros((len(df), 1))\n",
    "                        )\n",
    "                        y = df[target].values\n",
    "                        valid_mask = ~np.isnan(y)\n",
    "                        if sum(valid_mask) > 0:\n",
    "                            mi = mutual_info_regression(X[valid_mask], y[valid_mask])\n",
    "                            corr_dict[col] = np.mean(mi)\n",
    "                        else:\n",
    "                            corr_dict[col] = np.nan\n",
    "                    except:\n",
    "                        corr_dict[col] = np.nan\n",
    "\n",
    "            importance_df[f\"corr_with_{target}\"] = pd.Series(corr_dict)\n",
    "\n",
    "    # Outlier percentage for numerical columns\n",
    "    def outlier_percentage(column):\n",
    "        if np.issubdtype(column.dtype, np.number):\n",
    "            q1 = column.quantile(0.25)\n",
    "            q3 = column.quantile(0.75)\n",
    "            iqr = q3 - q1\n",
    "            lower_bound = q1 - 1.5 * iqr\n",
    "            upper_bound = q3 + 1.5 * iqr\n",
    "            outliers = ((column < lower_bound) | (column > upper_bound)).sum()\n",
    "            return (outliers / len(column)) * 100\n",
    "        return np.nan\n",
    "\n",
    "    importance_df[\"outlier_percentage\"] = df.apply(outlier_percentage)\n",
    "\n",
    "    return importance_df\n",
    "\n",
    "\n",
    "# Calculate column importance\n",
    "importance_results = analyze_column_importance(df)\n",
    "\n",
    "# Add composite score based on all metrics\n",
    "# Normalize each metric between 0 and 1\n",
    "normalized_df = importance_results.copy()\n",
    "for col in normalized_df.columns:\n",
    "    if col != \"missing_percentage\" and col != \"outlier_percentage\":\n",
    "        if normalized_df[col].notna().sum() > 0:\n",
    "            min_val = normalized_df[col].min()\n",
    "            max_val = normalized_df[col].max()\n",
    "            if max_val > min_val:\n",
    "                normalized_df[col] = (normalized_df[col] - min_val) / (\n",
    "                    max_val - min_val\n",
    "                )\n",
    "    else:\n",
    "        # For missing and outlier percentages, lower is better\n",
    "        if normalized_df[col].notna().sum() > 0:\n",
    "            min_val = normalized_df[col].min()\n",
    "            max_val = normalized_df[col].max()\n",
    "            if max_val > min_val:\n",
    "                normalized_df[col] = 1 - (\n",
    "                    (normalized_df[col] - min_val) / (max_val - min_val)\n",
    "                )\n",
    "\n",
    "# Calculate composite score (weighted average of normalized metrics)\n",
    "weights = {\n",
    "    \"cardinality_ratio\": 0.15,\n",
    "    \"entropy\": 0.2,\n",
    "    \"avg_word_count\": 0.2,\n",
    "    \"missing_percentage\": 0.15,\n",
    "    \"outlier_percentage\": 0.1,\n",
    "}\n",
    "\n",
    "# Add weights for correlation columns\n",
    "corr_cols = [col for col in normalized_df.columns if col.startswith(\"corr_with_\")]\n",
    "if corr_cols:\n",
    "    corr_weight = 0.2 / len(corr_cols)\n",
    "    for col in corr_cols:\n",
    "        weights[col] = corr_weight\n",
    "\n",
    "# Calculate composite score\n",
    "normalized_df[\"composite_score\"] = 0\n",
    "for col, weight in weights.items():\n",
    "    if col in normalized_df.columns:\n",
    "        normalized_df[\"composite_score\"] += normalized_df[col].fillna(0) * weight\n",
    "\n",
    "# Get top columns by composite score\n",
    "top_columns = normalized_df.sort_values(\"composite_score\", ascending=False)\n",
    "\n",
    "print(\"Top 10 columns by importance score:\")\n",
    "print(top_columns[\"composite_score\"].head(10))\n",
    "\n",
    "# Extract top 5 critical columns\n",
    "critical_columns = top_columns.index[:5].tolist()\n",
    "print(\"\\nIdentified 5 most critical columns:\")\n",
    "print(critical_columns)\n",
    "\n",
    "# Detailed justification for each critical column\n",
    "print(\"\\nJustification for critical column selection:\")\n",
    "for col in critical_columns:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(\n",
    "        f\"  - Missing values: {importance_results.loc[col, 'missing_percentage']:.2f}%\"\n",
    "    )\n",
    "    if \"avg_word_count\" in importance_results.columns and not pd.isna(\n",
    "        importance_results.loc[col, \"avg_word_count\"]\n",
    "    ):\n",
    "        print(\n",
    "            f\"  - Average word count: {importance_results.loc[col, 'avg_word_count']:.2f}\"\n",
    "        )\n",
    "    if \"cardinality\" in importance_results.columns:\n",
    "        print(f\"  - Unique values: {importance_results.loc[col, 'cardinality']}\")\n",
    "    for target in [\"REPAIR_AGE\", \"TOTALCOST\", \"LBRCOST\"]:\n",
    "        corr_col = f\"corr_with_{target}\"\n",
    "        if corr_col in importance_results.columns and not pd.isna(\n",
    "            importance_results.loc[col, corr_col]\n",
    "        ):\n",
    "            print(\n",
    "                f\"  - Correlation/MI with {target}: {importance_results.loc[col, corr_col]:.4f}\"\n",
    "            )\n",
    "    print(\n",
    "        f\"  - Composite importance score: {normalized_df.loc[col, 'composite_score']:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputed 100 missing values in CAMPAIGN_NBR with median nan\n",
      "Imputed 0 missing values in CAUSAL_PART_NM with mode 'WHEEL ASM-STRG *JET BLACK'\n",
      "Imputed 0 missing values in PLANT with mode 'SIL'\n",
      "Imputed 0 missing values in STATE with mode 'CA'\n",
      "Imputed 0 missing values in REPAIR_DLR_POSTAL_CD with mode '907551909'\n",
      "Imputed 0 missing values in VEH_TEST_GRP with mode 'T05.3386'\n",
      "Imputed 0 missing values in OPTN_FAMLY_CERTIFICATION with mode 'FE9'\n",
      "Imputed 0 missing values in OPTF_FAMLY_EMISSIOF_SYSTEM with mode 'FTB'\n",
      "Imputed 0 missing values in ENGINE_SOURCE_PLANT with mode '830107152'\n",
      "Imputed 0 missing values in ENGINE_TRACE_NBR with mode 'V2210281MFTX0488'\n",
      "Imputed 0 missing values in TRANSMISSION_TRACE_NBR with mode '21210129IKBP0429'\n",
      "Imputed 0 missing values in LINE_SERIES with mode '1500'\n",
      "Dataset shape after cleaning: (100, 73)\n",
      "Missing values after cleaning: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\#VSCode\\Recommender\\env\\Lib\\site-packages\\numpy\\lib\\_nanfunctions_impl.py:1215: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "C:\\Users\\tirtn\\AppData\\Local\\Temp\\ipykernel_38328\\4266995265.py:23: UserWarning: Unable to sort modes: '<' not supported between instances of 'int' and 'str'\n",
      "  mode_val = df_clean[col].mode()[0]\n"
     ]
    }
   ],
   "source": [
    "def clean_data(df):\n",
    "    \"\"\"\n",
    "    Clean the dataset by handling missing values, inconsistencies, and outliers\n",
    "    \"\"\"\n",
    "    df_clean = df.copy()\n",
    "\n",
    "    # Handle missing values\n",
    "    numerical_cols = df_clean.select_dtypes(include=np.number).columns\n",
    "    categorical_cols = df_clean.select_dtypes(include=[\"object\"]).columns\n",
    "\n",
    "    # For numerical columns, impute with median\n",
    "    for col in numerical_cols:\n",
    "        if df_clean[col].isnull().sum() > 0:\n",
    "            median_val = df_clean[col].median()\n",
    "            df_clean[col] = df_clean[col].fillna(median_val)\n",
    "            print(\n",
    "                f\"Imputed {df_clean[col].isnull().sum()} missing values in {col} with median {median_val:.2f}\"\n",
    "            )\n",
    "\n",
    "    # For categorical columns, impute with mode\n",
    "    for col in categorical_cols:\n",
    "        if df_clean[col].isnull().sum() > 0:\n",
    "            mode_val = df_clean[col].mode()[0]\n",
    "            df_clean[col] = df_clean[col].fillna(mode_val)\n",
    "            print(\n",
    "                f\"Imputed {df_clean[col].isnull().sum()} missing values in {col} with mode '{mode_val}'\"\n",
    "            )\n",
    "\n",
    "    # Standardize text in categorical columns (lowercase and trim)\n",
    "    for col in categorical_cols:\n",
    "        df_clean[col] = df_clean[col].astype(str).str.strip().str.lower()\n",
    "\n",
    "    # Handle inconsistencies in categorical columns\n",
    "    # For example, standardize common typos or variations\n",
    "    # This would be more specific based on the actual data patterns\n",
    "\n",
    "    # Handle outliers in numerical columns\n",
    "    # Instead of removing, we'll create flags (already present as _OUTLIER columns)\n",
    "    # We can use these flags in our analysis\n",
    "\n",
    "    return df_clean\n",
    "\n",
    "\n",
    "# Clean the data\n",
    "df_clean = clean_data(df)\n",
    "\n",
    "# Check the consistency of the data after cleaning\n",
    "print(\"Dataset shape after cleaning:\", df_clean.shape)\n",
    "print(\"Missing values after cleaning:\", df_clean.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Ensure output directory exists\n",
    "IMG_DIR = \"img\"\n",
    "os.makedirs(IMG_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# 1. Visualize Column Importance\n",
    "def visualize_column_importance(importance_df, top_n=10):\n",
    "    \"\"\"\n",
    "    Create visualizations for column importance\n",
    "    \"\"\"\n",
    "    # Horizontal bar chart for column importance\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_cols = importance_df.sort_values(\"composite_score\", ascending=False).head(top_n)\n",
    "    sns.barplot(x=top_cols[\"composite_score\"], y=top_cols.index)\n",
    "    plt.title(\"Columns by Importance Score\")\n",
    "    plt.xlabel(\"Importance Score\")\n",
    "    plt.ylabel(\"Column Name\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{IMG_DIR}/column_importance.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Heatmap of important numerical columns\n",
    "    num_cols = df.select_dtypes(include=np.number).columns\n",
    "    top_num_cols = [col for col in top_cols.index if col in num_cols]\n",
    "    if len(top_num_cols) > 1:\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        correlation_matrix = df[top_num_cols].corr()\n",
    "        sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", vmin=-1, vmax=1)\n",
    "        plt.title(\"Correlation Matrix of Important Numerical Columns\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{IMG_DIR}/correlation_heatmap.png\")\n",
    "        plt.close()\n",
    "\n",
    "    # Word clouds for key text columns\n",
    "    text_cols = [\n",
    "        \"CUSTOMER_VERBATIM\",\n",
    "        \"CORRECTION_VERBATIM\",\n",
    "        \"GLOBAL_LABOR_CODE_DESCRIPTION\",\n",
    "    ]\n",
    "    for col in text_cols:\n",
    "        if col in df.columns:\n",
    "            text = \" \".join(df[col].dropna().astype(str))\n",
    "            if len(text) > 100:\n",
    "                plt.figure(figsize=(10, 8))\n",
    "                wordcloud = WordCloud(\n",
    "                    width=800,\n",
    "                    height=800,\n",
    "                    background_color=\"white\",\n",
    "                    max_words=200,\n",
    "                    contour_width=3,\n",
    "                    contour_color=\"steelblue\",\n",
    "                ).generate(text)\n",
    "                plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "                plt.axis(\"off\")\n",
    "                plt.title(f\"Word Cloud - {col.replace('_', ' ')}\")\n",
    "                plt.tight_layout(pad=0)\n",
    "                plt.savefig(f\"{IMG_DIR}/wordcloud_{col.replace('_', ' ')}.png\")\n",
    "                plt.close()\n",
    "\n",
    "\n",
    "# 2. Visualize Tag Distributions\n",
    "def visualize_tag_distributions(df):\n",
    "    \"\"\"\n",
    "    Visualize tag distributions (splitting comma-separated values)\n",
    "    \"\"\"\n",
    "    tag_columns = [\n",
    "        \"ISSUES\",\n",
    "        \"COMPONENTS\",\n",
    "        \"ACTIONS\",\n",
    "        \"REPAIR_COMPLEXITY\",\n",
    "        \"VEHICLE_SYSTEM\",\n",
    "        \"FAILURE_MODE\",\n",
    "        \"REPAIR_URGENCY\",\n",
    "    ]\n",
    "\n",
    "    for col in tag_columns:\n",
    "        if col in df.columns:\n",
    "            all_tags = (\n",
    "                df[col]\n",
    "                .dropna()\n",
    "                .astype(str)\n",
    "                .apply(lambda x: [tag.strip() for tag in x.split(\",\")])\n",
    "            )\n",
    "            flat_tags = [tag for sublist in all_tags for tag in sublist if tag]\n",
    "            tag_counts = pd.Series(flat_tags).value_counts().nlargest(10)\n",
    "\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            sns.barplot(x=tag_counts.values, y=tag_counts.index)\n",
    "            plt.title(col.replace(\"_\", \" \"))\n",
    "            plt.xlabel(\"Count\")\n",
    "            plt.ylabel(col.replace(\"_\", \" \"))\n",
    "            plt.tight_layout(pad=2.0)\n",
    "            plt.savefig(f\"{IMG_DIR}/tag_distribution_{col}.png\")\n",
    "            plt.close()\n",
    "\n",
    "\n",
    "# 3. Visualize Repair Patterns\n",
    "def visualize_repair_patterns(df):\n",
    "    \"\"\"\n",
    "    Visualize repair patterns and relationships\n",
    "    \"\"\"\n",
    "    # Scatter plot: Repair Age vs Cost\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(df[\"REPAIR_AGE\"], df[\"TOTALCOST\"], alpha=0.5)\n",
    "    plt.title(\"Repair Age vs Total Cost\")\n",
    "    plt.xlabel(\"Repair Age\")\n",
    "    plt.ylabel(\"Total Cost\")\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{IMG_DIR}/repair_age_vs_cost.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Box plot: Cost by Repair Complexity\n",
    "    if \"REPAIR_COMPLEXITY\" in df.columns:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.boxplot(x=\"REPAIR_COMPLEXITY\", y=\"TOTALCOST\", data=df)\n",
    "        plt.title(\"Total Cost by Repair Complexity\")\n",
    "        plt.xlabel(\"Repair Complexity\")\n",
    "        plt.ylabel(\"Total Cost\")\n",
    "        plt.xticks(rotation=0, ha=\"right\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{IMG_DIR}/cost_by_complexity.png\")\n",
    "        plt.close()\n",
    "\n",
    "    # Box plot: Cost by Vehicle System\n",
    "    if \"VEHICLE_SYSTEM\" in df.columns:\n",
    "        top_systems = df[\"VEHICLE_SYSTEM\"].value_counts().nlargest(5).index\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.boxplot(\n",
    "            x=\"VEHICLE_SYSTEM\",\n",
    "            y=\"TOTALCOST\",\n",
    "            data=df[df[\"VEHICLE_SYSTEM\"].isin(top_systems)],\n",
    "        )\n",
    "        plt.title(\"Total Cost by Vehicle System\")\n",
    "        plt.xlabel(\"Vehicle System\")\n",
    "        plt.ylabel(\"Total Cost\")\n",
    "        plt.xticks(rotation=0, ha=\"right\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{IMG_DIR}/cost_by_system.png\")\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "# 4. Visualize Geographic Distribution\n",
    "def visualize_geographic_distribution(df):\n",
    "    \"\"\"\n",
    "    Visualize geographic distribution of repairs\n",
    "    \"\"\"\n",
    "    if \"STATE\" in df.columns:\n",
    "        # State-wise repair count\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        state_counts = df[\"STATE\"].value_counts().nlargest(10)\n",
    "        sns.barplot(x=state_counts.index, y=state_counts.values)\n",
    "        plt.title(\"State-wise Repair Count\")\n",
    "        plt.xlabel(\"State\")\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.xticks(rotation=0, ha=\"right\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{IMG_DIR}/repairs_by_state.png\")\n",
    "        plt.close()\n",
    "\n",
    "        # State-wise average cost\n",
    "        state_avg_cost = df.groupby(\"STATE\")[\"TOTALCOST\"].mean().nlargest(10)\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.barplot(x=state_avg_cost.index, y=state_avg_cost.values)\n",
    "        plt.title(\"State-wise Average Repair Cost\")\n",
    "        plt.xlabel(\"State\")\n",
    "        plt.ylabel(\"Average Cost\")\n",
    "        plt.xticks(rotation=0, ha=\"right\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{IMG_DIR}/avg_cost_by_state.png\")\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "# 5. Visualize Platform and Engine Distribution\n",
    "def visualize_platform_engine(df):\n",
    "    \"\"\"\n",
    "    Visualize distribution of platform and engine types\n",
    "    \"\"\"\n",
    "    # Platform distribution\n",
    "    if \"PLATFORM\" in df.columns:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        platform_counts = df[\"PLATFORM\"].value_counts().nlargest(10)\n",
    "        sns.barplot(x=platform_counts.index, y=platform_counts.values)\n",
    "        plt.title(\"Platform-wise Repair Count\")\n",
    "        plt.xlabel(\"Platform\")\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.xticks(rotation=0, ha=\"right\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{IMG_DIR}/repairs_by_platform.png\")\n",
    "        plt.close()\n",
    "\n",
    "    # Engine type distribution\n",
    "    if \"ENGINE\" in df.columns:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        engine_counts = df[\"ENGINE\"].value_counts().nlargest(10)\n",
    "        sns.barplot(x=engine_counts.index, y=engine_counts.values)\n",
    "        plt.title(\"Engine Type-wise Repair Count\")\n",
    "        plt.xlabel(\"Engine Type\")\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.xticks(rotation=0, ha=\"right\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{IMG_DIR}/repairs_by_engine.png\")\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "# Final Calls to Generate All Visualizations\n",
    "visualize_column_importance(normalized_df)\n",
    "visualize_tag_distributions(df_clean)\n",
    "visualize_repair_patterns(df_clean)\n",
    "visualize_geographic_distribution(df_clean)\n",
    "visualize_platform_engine(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_DIR = \"img2\"\n",
    "os.makedirs(IMG_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "def analyze_tag_relationships(df):\n",
    "    \"\"\"\n",
    "    Analyze relationships between tags and create visualizations\n",
    "    \"\"\"\n",
    "    # 1. Co-occurrence matrix for issues and components\n",
    "    if all(col in df.columns for col in [\"ISSUES\", \"COMPONENTS\"]):\n",
    "        # Split comma-separated values, flatten, and count\n",
    "        issue_tags = (\n",
    "            df[\"ISSUES\"]\n",
    "            .dropna()\n",
    "            .astype(str)\n",
    "            .apply(lambda x: [tag.strip() for tag in x.split(\",\")])\n",
    "        )\n",
    "        component_tags = (\n",
    "            df[\"COMPONENTS\"]\n",
    "            .dropna()\n",
    "            .astype(str)\n",
    "            .apply(lambda x: [tag.strip() for tag in x.split(\",\")])\n",
    "        )\n",
    "\n",
    "        # Get top tags individually\n",
    "        flat_issues = [tag for sublist in issue_tags for tag in sublist]\n",
    "        flat_components = [tag for sublist in component_tags for tag in sublist]\n",
    "        top_issues = pd.Series(flat_issues).value_counts().nlargest(10).index\n",
    "        top_components = pd.Series(flat_components).value_counts().nlargest(10).index\n",
    "\n",
    "        # Initialize co-occurrence matrix\n",
    "        cooccurrence = pd.DataFrame(index=top_issues, columns=top_components, data=0)\n",
    "\n",
    "        for i, row in df.iterrows():\n",
    "            row_issues = [tag.strip() for tag in str(row.get(\"ISSUES\", \"\")).split(\",\")]\n",
    "            row_components = [\n",
    "                tag.strip() for tag in str(row.get(\"COMPONENTS\", \"\")).split(\",\")\n",
    "            ]\n",
    "            for issue in row_issues:\n",
    "                for component in row_components:\n",
    "                    if issue in top_issues and component in top_components:\n",
    "                        cooccurrence.loc[issue, component] += 1\n",
    "\n",
    "        # Create heatmap\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        sns.heatmap(cooccurrence, annot=True, cmap=\"YlGnBu\", fmt=\"d\")\n",
    "        plt.title(\"Co-occurrence of Issues and Components\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{IMG_DIR}/issue_component_cooccurrence.png\")\n",
    "        plt.close()\n",
    "\n",
    "    # 2. Average repair cost by component and complexity\n",
    "    if all(\n",
    "        col in df.columns for col in [\"COMPONENTS\", \"REPAIR_COMPLEXITY\", \"TOTALCOST\"]\n",
    "    ):\n",
    "        component_tags = (\n",
    "            df[\"COMPONENTS\"]\n",
    "            .dropna()\n",
    "            .astype(str)\n",
    "            .apply(lambda x: [tag.strip() for tag in x.split(\",\")])\n",
    "        )\n",
    "        flat_components = [tag for sublist in component_tags for tag in sublist]\n",
    "        top_components = pd.Series(flat_components).value_counts().nlargest(8).index\n",
    "\n",
    "        # Expand rows for multiple components\n",
    "        expanded_rows = []\n",
    "        for _, row in df.iterrows():\n",
    "            components = [\n",
    "                tag.strip() for tag in str(row.get(\"COMPONENTS\", \"\")).split(\",\")\n",
    "            ]\n",
    "            for comp in components:\n",
    "                if comp in top_components:\n",
    "                    expanded_rows.append(\n",
    "                        {\n",
    "                            \"COMPONENTS\": comp,\n",
    "                            \"REPAIR_COMPLEXITY\": row[\"REPAIR_COMPLEXITY\"],\n",
    "                            \"TOTALCOST\": row[\"TOTALCOST\"],\n",
    "                        }\n",
    "                    )\n",
    "        cost_df = pd.DataFrame(expanded_rows)\n",
    "\n",
    "        # Plot grouped bar chart\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        sns.barplot(\n",
    "            x=\"COMPONENTS\", y=\"TOTALCOST\", hue=\"REPAIR_COMPLEXITY\", data=cost_df\n",
    "        )\n",
    "        plt.title(\"Average Repair Cost by Component and Complexity\")\n",
    "        plt.xlabel(\"Component\")\n",
    "        plt.ylabel(\"Average Cost\")\n",
    "        plt.xticks(rotation=45, ha=\"right\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{IMG_DIR}/cost_by_component_complexity.png\")\n",
    "        plt.close()\n",
    "\n",
    "    # 3. Relationship between repair age and repair complexity\n",
    "    if all(col in df.columns for col in [\"REPAIR_AGE\", \"REPAIR_COMPLEXITY\"]):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.boxplot(x=\"REPAIR_COMPLEXITY\", y=\"REPAIR_AGE\", data=df)\n",
    "        plt.title(\"Repair Age by Complexity\")\n",
    "        plt.xlabel(\"Repair Complexity\")\n",
    "        plt.ylabel(\"Repair Age\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{IMG_DIR}/repair_age_by_complexity.png\")\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "# Call to run\n",
    "analyze_tag_relationships(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Critical Columns Analysis ===\n",
      "The top 5 most important columns identified are: CORRECTION_VERBATIM, CUSTOMER_VERBATIM, VIN, LAST_KNOWN_DLR_CITY, REPAIRING_DEALER_CODE\n",
      "- CORRECTION VERBATIM: Importance score: 0.7333\n",
      "  - Contains rich textual information about repair issues and actions\n",
      "  - Average word count: 1.00\n",
      "- CUSTOMER VERBATIM: Importance score: 0.6682\n",
      "  - Contains rich textual information about repair issues and actions\n",
      "  - Average word count: 0.84\n",
      "- VIN: Importance score: 0.5446\n",
      "- LAST KNOWN DLR CITY: Importance score: 0.5403\n",
      "- REPAIRING DEALER CODE: Importance score: 0.5382\n",
      "\n",
      "=== Tag Analysis ===\n",
      "- Top ISSUES:\n",
      "  - material problems: 30 occurrences (30.0%)\n",
      "  - heating malfunctions: 24 occurrences (24.0%)\n",
      "  - stitching failures: 11 occurrences (11.0%)\n",
      "- Top COMPONENTS:\n",
      "  - steering wheel (main component): 93 occurrences (93.0%)\n",
      "  - leather/material covering: 23 occurrences (23.0%)\n",
      "  - heated module: 21 occurrences (21.0%)\n",
      "- Top ACTIONS:\n",
      "  - part replacement: 91 occurrences (91.0%)\n",
      "  - pre-authorization (pra): 26 occurrences (26.0%)\n",
      "  - system verification: 15 occurrences (15.0%)\n",
      "- Top REPAIR COMPLEXITY:\n",
      "  - medium: 83 occurrences (83.0%)\n",
      "  - high: 15 occurrences (15.0%)\n",
      "  - low: 2 occurrences (2.0%)\n",
      "- Top VEHICLE SYSTEM:\n",
      "  - steering system: 47 occurrences (47.0%)\n",
      "  - comfort system: 22 occurrences (22.0%)\n",
      "  - interior trim: 18 occurrences (18.0%)\n",
      "- Top FAILURE MODE:\n",
      "  - unknown: 33 occurrences (33.0%)\n",
      "  - material degradation: 25 occurrences (25.0%)\n",
      "  - functional failure: 14 occurrences (14.0%)\n",
      "- Top REPAIR URGENCY:\n",
      "  - normal: 52 occurrences (52.0%)\n",
      "  - early failure: 25 occurrences (25.0%)\n",
      "  - long-term issue: 16 occurrences (16.0%)\n",
      "\n",
      "=== Cost Analysis ===\n",
      "- Average total cost: $554.93\n",
      "- Median total cost: $457.23\n",
      "- Cost range: $27.69 to $3205.45\n",
      "- Average labor cost ratio: 27.2% of total cost\n",
      "- Average cost by complexity:\n",
      "  - high: $1154.95\n",
      "  - medium: $459.15\n",
      "  - low: $29.65\n",
      "\n",
      "=== Vehicle System Analysis ===\n",
      "- Most common vehicle systems requiring repair:\n",
      "  - steering system: 47 repairs (47.0%)\n",
      "  - comfort system: 22 repairs (22.0%)\n",
      "  - interior trim: 18 repairs (18.0%)\n",
      "  - driver assistance: 8 repairs (8.0%)\n",
      "  - electrical system: 5 repairs (5.0%)\n",
      "- Most expensive vehicle systems to repair:\n",
      "  - driver assistance: $1636.96 average cost\n",
      "  - electrical system: $509.22 average cost\n",
      "  - steering system: $475.32 average cost\n",
      "  - comfort system: $465.85 average cost\n",
      "  - interior trim: $403.45 average cost\n",
      "\n",
      "=== Geographic Analysis ===\n",
      "- States with most repairs:\n",
      "  - ca: 11 repairs (11.0%)\n",
      "  - fl: 9 repairs (9.0%)\n",
      "  - tx: 8 repairs (8.0%)\n",
      "  - oh: 6 repairs (6.0%)\n",
      "  - pa: 5 repairs (5.0%)\n",
      "- States with highest average repair costs:\n",
      "  - ms: $1474.06 average cost\n",
      "  - wa: $1439.65 average cost\n",
      "  - ny: $1113.22 average cost\n",
      "  - gj: $960.87 average cost\n",
      "  - il: $906.16 average cost\n"
     ]
    }
   ],
   "source": [
    "def generate_insights_summary(df, importance_df, critical_columns):\n",
    "    \"\"\"\n",
    "    Generate insights and summary from the analysis\n",
    "    \"\"\"\n",
    "    insights = []\n",
    "\n",
    "    # 1. Critical columns insights\n",
    "    insights.append(\"=== Critical Columns Analysis ===\")\n",
    "    insights.append(\n",
    "        f\"The top 5 most important columns identified are: {', '.join(critical_columns)}\"\n",
    "    )\n",
    "\n",
    "    for col in critical_columns:\n",
    "        score = (\n",
    "            importance_df.loc[col, \"composite_score\"]\n",
    "            if \"composite_score\" in importance_df.columns\n",
    "            else \"N/A\"\n",
    "        )\n",
    "        if isinstance(score, (int, float)):\n",
    "            insights.append(f\"- {col.replace('_', ' ')}: Importance score: {score:.4f}\")\n",
    "        else:\n",
    "            insights.append(f\"- {col.replace('_', ' ')}: Importance score: {score}\")\n",
    "\n",
    "        if col in [\"CUSTOMER_VERBATIM\", \"CORRECTION_VERBATIM\"]:\n",
    "            insights.append(\n",
    "                \"  - Contains rich textual information about repair issues and actions\"\n",
    "            )\n",
    "            if \"avg_word_count\" in importance_df.columns:\n",
    "                insights.append(\n",
    "                    f\"  - Average word count: {importance_df.loc[col, 'avg_word_count']:.2f}\"\n",
    "                )\n",
    "        elif col in [\"REPAIR_AGE\", \"KM\", \"TOTALCOST\", \"LBRCOST\"]:\n",
    "            insights.append(\n",
    "                \"  - Critical metric for repair analysis and cost assessment\"\n",
    "            )\n",
    "            for target in [\"REPAIR_AGE\", \"TOTALCOST\", \"LBRCOST\"]:\n",
    "                key = f\"corr_with_{target}\"\n",
    "                if target != col and key in importance_df.columns:\n",
    "                    insights.append(\n",
    "                        f\"  - Correlation with {target.replace('_', ' ')}: {importance_df.loc[col, key]:.4f}\"\n",
    "                    )\n",
    "\n",
    "    # 2. Tag distribution insights\n",
    "    insights.append(\"\\n=== Tag Analysis ===\")\n",
    "    tag_columns = [\n",
    "        \"ISSUES\",\n",
    "        \"COMPONENTS\",\n",
    "        \"ACTIONS\",\n",
    "        \"REPAIR_COMPLEXITY\",\n",
    "        \"VEHICLE_SYSTEM\",\n",
    "        \"FAILURE_MODE\",\n",
    "        \"REPAIR_URGENCY\",\n",
    "    ]\n",
    "\n",
    "    for col in tag_columns:\n",
    "        if col in df.columns:\n",
    "            all_tags = (\n",
    "                df[col]\n",
    "                .dropna()\n",
    "                .astype(str)\n",
    "                .apply(lambda x: [tag.strip() for tag in x.split(\",\")])\n",
    "            )\n",
    "            flat_tags = [tag for sublist in all_tags for tag in sublist if tag]\n",
    "            top_values = pd.Series(flat_tags).value_counts().nlargest(3)\n",
    "            insights.append(f\"- Top {col.replace('_', ' ')}:\")\n",
    "            for val, count in top_values.items():\n",
    "                insights.append(\n",
    "                    f\"  - {val}: {count} occurrences ({count / len(df) * 100:.1f}%)\"\n",
    "                )\n",
    "\n",
    "    # 3. Cost analysis insights\n",
    "    insights.append(\"\\n=== Cost Analysis ===\")\n",
    "    if \"TOTALCOST\" in df.columns:\n",
    "        insights.append(f\"- Average total cost: ${df['TOTALCOST'].mean():.2f}\")\n",
    "        insights.append(f\"- Median total cost: ${df['TOTALCOST'].median():.2f}\")\n",
    "        insights.append(\n",
    "            f\"- Cost range: ${df['TOTALCOST'].min():.2f} to ${df['TOTALCOST'].max():.2f}\"\n",
    "        )\n",
    "\n",
    "    if \"LBRCOST\" in df.columns and \"TOTALCOST\" in df.columns:\n",
    "        labor_ratio = (df[\"LBRCOST\"] / df[\"TOTALCOST\"]).replace(\n",
    "            [np.inf, -np.inf], np.nan\n",
    "        ).dropna().mean() * 100\n",
    "        insights.append(f\"- Average labor cost ratio: {labor_ratio:.1f}% of total cost\")\n",
    "\n",
    "    if \"REPAIR_COMPLEXITY\" in df.columns and \"TOTALCOST\" in df.columns:\n",
    "        complexity_cost = (\n",
    "            df.groupby(\"REPAIR_COMPLEXITY\")[\"TOTALCOST\"]\n",
    "            .mean()\n",
    "            .sort_values(ascending=False)\n",
    "        )\n",
    "        insights.append(\"- Average cost by complexity:\")\n",
    "        for complexity, cost in complexity_cost.items():\n",
    "            insights.append(f\"  - {complexity}: ${cost:.2f}\")\n",
    "\n",
    "    # 4. Vehicle system insights\n",
    "    insights.append(\"\\n=== Vehicle System Analysis ===\")\n",
    "    if \"VEHICLE_SYSTEM\" in df.columns:\n",
    "        system_counts = df[\"VEHICLE_SYSTEM\"].value_counts().nlargest(5)\n",
    "        insights.append(\"- Most common vehicle systems requiring repair:\")\n",
    "        for system, count in system_counts.items():\n",
    "            insights.append(\n",
    "                f\"  - {system}: {count} repairs ({count / len(df) * 100:.1f}%)\"\n",
    "            )\n",
    "\n",
    "    if \"VEHICLE_SYSTEM\" in df.columns and \"TOTALCOST\" in df.columns:\n",
    "        system_costs = df.groupby(\"VEHICLE_SYSTEM\")[\"TOTALCOST\"].mean().nlargest(5)\n",
    "        insights.append(\"- Most expensive vehicle systems to repair:\")\n",
    "        for system, cost in system_costs.items():\n",
    "            insights.append(f\"  - {system}: ${cost:.2f} average cost\")\n",
    "\n",
    "    # 5. Geographic insights\n",
    "    insights.append(\"\\n=== Geographic Analysis ===\")\n",
    "    if \"STATE\" in df.columns:\n",
    "        state_counts = df[\"STATE\"].value_counts().nlargest(5)\n",
    "        insights.append(\"- States with most repairs:\")\n",
    "        for state, count in state_counts.items():\n",
    "            insights.append(\n",
    "                f\"  - {state}: {count} repairs ({count / len(df) * 100:.1f}%)\"\n",
    "            )\n",
    "\n",
    "        if \"TOTALCOST\" in df.columns:\n",
    "            state_costs = df.groupby(\"STATE\")[\"TOTALCOST\"].mean().nlargest(5)\n",
    "            insights.append(\"- States with highest average repair costs:\")\n",
    "            for state, cost in state_costs.items():\n",
    "                insights.append(f\"  - {state}: ${cost:.2f} average cost\")\n",
    "\n",
    "    # Print all insights\n",
    "    print(\"\\n\".join(insights))\n",
    "\n",
    "\n",
    "# Generate insights summary\n",
    "generate_insights_summary(df_clean, normalized_df, critical_columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
